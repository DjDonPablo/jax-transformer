# jax-transformer

Work in progress...

Ressources to create this tranformer :
- Attention is all you need : https://arxiv.org/pdf/1706.03762
- The neural network serie from the goat 3blue1brown : https://www.youtube.com/@3blue1brown
- Some articles to clarify some points

TODO :
- add -max in softmax for numerical stability
- masking on gradients when padded sequence
- new dataset
- don't recompute all attention matrix at inference
